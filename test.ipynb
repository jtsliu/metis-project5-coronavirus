{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# text preprocessing\n",
    "import preprocessor as p\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "\n",
    "# visualization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import KMeans \n",
    "\n",
    "import umap.umap_ as umap\n",
    "from yellowbrick.text import UMAPVisualizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "# [stop_words.append(x.replace('\\'', '')) for x in stop_words if \"'\" in x]\n",
    "stop_words.extend(['coronavirus', 'covid', 'covidãƒ¼', 'coronavirusoutbreak', 'coronaviruspandemic'])\n",
    "stop_words = set(stop_words)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "    preprocesser = lambda x: p.clean(x) #removes URL, @Mentions, Emojis, Smileys\n",
    "    \n",
    "    # prevent acronym for United States from losing meaning\n",
    "    expand_us = lambda x: x.replace('U.S.', 'United States')\n",
    "    \n",
    "    # remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    remove_punctuation = lambda x: x.translate(table)\n",
    "    \n",
    "    # remove numbers\n",
    "    text_nonum = lambda x: re.sub(r'\\d+', '', x)\n",
    "    \n",
    "    # convert all letters to lowercase\n",
    "    text_lower = lambda x: x.lower()\n",
    "\n",
    "    # substitute multiple spaces with single space\n",
    "    text_nospaces = lambda x: re.sub(r'\\s+', ' ', x, flags=re.I)\n",
    "\n",
    "    # remove all single characters\n",
    "    text_single = lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)\n",
    "    \n",
    "    # tokenize words\n",
    "    tokenize = lambda x: word_tokenize(x)\n",
    "    remove_stop = lambda x: [w for w in x if w not in stop_words]\n",
    "#     lemmatize_tweet = lambda x: [lemmatizer.lemmatize(word) for word in x]\n",
    "    create_string = lambda x: ' '.join(x)\n",
    "    \n",
    "    for function in [preprocesser, expand_us, remove_punctuation, \n",
    "                     text_nonum, text_lower, text_nospaces, \n",
    "                     text_single,\n",
    "                     tokenize, remove_stop, \n",
    "                     # lemmatize_tweet, \n",
    "                     create_string\n",
    "                    ]:\n",
    "        text = text.map(function)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['full_text'] = cleanText(new_df.full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\", topic_names[ix], \"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                         for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = [f'component_{x}' for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf, dtm = createCustomTFIDFvectorizer(new_df, n_range=(1,2), stop = stop_words)\n",
    "\n",
    "# don't use TF-IDF for tweets\n",
    "# use CV because it's a small doc\n",
    "# suggestion - binary = True ~ can throw in a lot of noise\n",
    "cv = CountVectorizer(ngram_range = (1,1), stop_words = stop_words, min_df=3, max_df=0.85)\n",
    "data_dtm = cv.fit_transform(new_df.full_text)\n",
    "data_dtm = pd.DataFrame(data_dtm.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = new_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
