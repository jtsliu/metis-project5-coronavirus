{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T18:24:19.195747Z",
     "start_time": "2020-03-17T18:24:19.040903Z"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from credentials import *\n",
    "\n",
    "# Create credentials.py in working directory with \n",
    "# your application's key and secret. API_KEY and API_SECRET\n",
    "auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {'california': ,\n",
    "           'florida': ,\n",
    "           'massachusetts': ,\n",
    "           'newyork': ,\n",
    "           'washington':,\n",
    "           'tennessee':,   \n",
    "           'texas':,\n",
    "           'louisiana':,\n",
    "           'illinois':,\n",
    "           'georgia':,\n",
    "           'idaho':,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What’s more using the initial values for max_id and/or since_id you can fetch results to and from arbitrary IDs. This is really helpful if you want to the program repeatedly to fetch newer results since last run. Just look up the max ID (the ID of the first line) from the previous run and set that to since_id for the next run. If you’ve to stop your program before exhausting all the possible results and rerun it again to fetch the remaining results, you can look up the min ID (the ID of the last line) and pass that as max_id for the next run to start from that ID and below.\n",
    "* max_id (first tweet) from previous run -> since_id = \n",
    "* min_id (last tweet) -> max_id for new run if starting from where you left off = \n",
    "\n",
    "**California**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 298499 tweets**\n",
    "\n",
    "* searchQuery = 'covid19'\n",
    "* **Downloaded 57200 tweets**\n",
    "* tweetLocation = '36.116,-119.682,300mi'\n",
    "\n",
    "**Florida**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 102359 tweets**\n",
    "\n",
    "* searchQuery = 'covid19'\n",
    "* **Downloaded --- tweets**\n",
    "* tweetLocation = '27.766,-81.687,225mi'\n",
    "\n",
    "**Massachusetts**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 94157 tweets**\n",
    "* tweetLocation = '42.230,-71.530,100mi'\n",
    "\n",
    "**New York**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 293528 tweets**\n",
    "* tweetLocation =  '40.700,-73.974,50mi'\n",
    "\n",
    "**Washington**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 85792 tweets**\n",
    "* tweetLocation = '47.401,-121.491,200mi'\n",
    "\n",
    "**Tennessee**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 139487 tweets**\n",
    "* tweetLocation = '35.748,-86.692,200mi'\n",
    "\n",
    "**Texas**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 196317 tweets**\n",
    "* tweetLocation = '31.527,-99.524,350mi'\n",
    "\n",
    "**Louisiana**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 54235 tweets**\n",
    "* tweetLocation = '31.170,-91.868,150mi'\n",
    "\n",
    "**Illinois**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 140102 tweets**\n",
    "* tweetLocation = '40.350,-88.986,150mi'\n",
    "\n",
    "**Colorado**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 30059 tweets**\n",
    "* tweetLocation = '39.060,-105.311,200mi'\n",
    "\n",
    "**Georgia**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 101940 tweets**\n",
    "* tweetLocation = '32.781,-83.334,150mi'\n",
    "\n",
    "**Idaho**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 6882 tweets**\n",
    "* tweetLocation = '44.241,-114.479,200mi'\n",
    "\n",
    "Countries\n",
    "---\n",
    "**Italy**\n",
    "* **Downloaded 13835 tweets**\n",
    "* tweetLocation = '41.872,12.567,275mi'\n",
    "\n",
    "**Hubei, China**\n",
    "* **Downloaded 1359 tweets**\n",
    "* tweetLocation = '30.976,112.271,275mi'\n",
    "\n",
    "**South Korea**\n",
    "Latitude\tLongitude\n",
    "35.908\t127.767\n",
    "\n",
    "**France**\n",
    "Latitude\tLongitude\n",
    "46.228\t2.214\n",
    "\n",
    "\n",
    "**Spain**\n",
    "Latitude\tLongitude\n",
    "40.464\t-3.749\n",
    "\n",
    "**Germany**\n",
    "Latitude\tLongitude\n",
    "51.166\t10.452\n",
    "\n",
    "Retweets Included Below\n",
    "--- \n",
    "**California**\n",
    "* Downloaded 670881 tweets\n",
    "* max_id (first tweet) from previous run -> since_id = \"1239943180579000320\" \n",
    "* min_id (last tweet) -> max_id for new run if starting from where you left off = \"1239289843491823616\"\n",
    "* searchQuery = 'coronavirus'\n",
    "* tweetLocation = '36.116,-119.682,300mi'\n",
    "* tweetLang = 'en'\n",
    "* fName = 'coronavirus_california_tweets.txt'\n",
    "\n",
    "**Florida**\n",
    "* Downloaded\n",
    "* max_id (first tweet) from previous run -> since_id = \n",
    "* min_id (last tweet) -> max_id for new run if starting from where you left off = \n",
    "* searchQuery = 'coronavirus'\n",
    "* tweetLocation = '27.766,-81.687,225mi'\n",
    "* tweetLang = 'en'\n",
    "* fName = 'coronavirus_florida_tweets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T23:53:51.084223Z",
     "start_time": "2020-03-18T23:53:37.265340Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 10000000 tweets\n",
      "Downloaded 100 tweets\n",
      "Downloaded 200 tweets\n",
      "Downloaded 300 tweets\n",
      "Downloaded 399 tweets\n",
      "Downloaded 497 tweets\n",
      "Downloaded 568 tweets\n",
      "Downloaded 619 tweets\n",
      "Downloaded 665 tweets\n",
      "Downloaded 715 tweets\n",
      "Downloaded 761 tweets\n",
      "Downloaded 829 tweets\n",
      "Downloaded 907 tweets\n",
      "Downloaded 959 tweets\n",
      "Downloaded 1023 tweets\n",
      "Downloaded 1108 tweets\n",
      "Downloaded 1183 tweets\n",
      "Downloaded 1234 tweets\n",
      "Downloaded 1283 tweets\n",
      "Downloaded 1336 tweets\n",
      "Downloaded 1359 tweets\n",
      "No more tweets found\n",
      "Downloaded 1359 tweets, Saved to coronavirus_hubeichina_tweets.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import jsonpickle\n",
    "import os\n",
    "\n",
    "searchQuery = 'coronavirus -filter:replies -filter:retweets'  # this is what we're searching for\n",
    "tweetLocation = '30.976,112.271,275mi'\n",
    "tweetLang = 'en'\n",
    "\n",
    "maxTweets = 10000000 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "fName = f'coronavirus_hubeichina_tweets.txt' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "# else default to no lower limit, go as far back as API allows\n",
    "sinceId = None\n",
    "\n",
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1\n",
    "\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "with open(fName, 'w') as f:\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, geocode=tweetLocation, \n",
    "                                            lang=tweetLang, count=tweetsPerQry,\n",
    "                                            tweet_mode='extended')\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, geocode=tweetLocation, \n",
    "                                            lang=tweetLang, count=tweetsPerQry,\n",
    "                                            tweet_mode='extended',\n",
    "                                            since_id=sinceId)\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.search(q=searchQuery, geocode=tweetLocation, \n",
    "                                            lang=tweetLang, count=tweetsPerQry,\n",
    "                                            tweet_mode='extended',\n",
    "                                            max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = api.search(q=searchQuery, geocode=tweetLocation, \n",
    "                                            lang=tweetLang, count=tweetsPerQry,\n",
    "                                            tweet_mode='extended',\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "            if not new_tweets:\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "            for tweet in new_tweets:\n",
    "                f.write(jsonpickle.encode(tweet._json, unpicklable=False) +\n",
    "                        '\\n')\n",
    "            tweetCount += len(new_tweets)\n",
    "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n",
    "\n",
    "print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
