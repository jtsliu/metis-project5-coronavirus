{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.1-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U tweet-preprocessor\n",
    "#!pip install -U vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# text preprocessing\n",
    "import preprocessor as p\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "\n",
    "# sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['coronavirus', 'covid', 'covidー', 'coronavirusoutbreak', 'coronaviruspandemic'])\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    #removes URLs, @Mentions, Emojis, Smileys\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "    preprocesser = lambda x: p.clean(x) \n",
    "    \n",
    "    # prevent acronym for United States from losing meaning\n",
    "    expand_us = lambda x: x.replace('U.S.', 'United States')\n",
    "    \n",
    "    # remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    remove_punctuation = lambda x: x.translate(table)\n",
    "    \n",
    "    # remove numbers\n",
    "    text_nonum = lambda x: re.sub(r'\\d+', '', x)\n",
    "    \n",
    "    # convert all letters to lowercase\n",
    "    text_lower = lambda x: x.lower()\n",
    "\n",
    "    # substitute multiple spaces with single space\n",
    "    text_nospaces = lambda x: re.sub(r'\\s+', ' ', x, flags=re.I)\n",
    "\n",
    "    # remove all single characters\n",
    "    text_single = lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)\n",
    "    \n",
    "    # tokenize words\n",
    "    tokenize = lambda x: word_tokenize(x)\n",
    "    remove_stop = lambda x: [w for w in x if w not in stop_words]\n",
    "    lemmatize_tweet = lambda x: [lemmatizer.lemmatize(word) for word in x]\n",
    "    create_string = lambda x: ' '.join(x)\n",
    "    \n",
    "    for function in [preprocesser, expand_us, remove_punctuation, \n",
    "                     text_nonum, text_lower, text_nospaces, \n",
    "                     text_single,\n",
    "                     tokenize, remove_stop, \n",
    "                     lemmatize_tweet, \n",
    "                     create_string\n",
    "                    ]:\n",
    "        text = text.map(function)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = cleanText(df['full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment\n",
    "Following the typical threshold values as cited by the VADER documentation\n",
    "* positive sentiment: compound score >= 0.05\n",
    "* neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "* negative sentiment: compound score <= -0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_distribution'] = df['full_text'].apply(lambda x: analyser.polarity_scores(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment_distribution'].apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = ['california', 'colorado', 'florida', \n",
    "              'georgia', 'idaho', 'illinois', \n",
    "              'louisiana', 'massachusetts', 'newyork',\n",
    "              'tennessee', 'texas', 'washington']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing and analyzing sentiment for newyork!\n"
     ]
    }
   ],
   "source": [
    "for state in state_list:\n",
    "    df = pd.read_csv(f'tweets/tweets_updated/coronavirus_{state}_tweets_updated', engine = 'python')\n",
    "    df['processed_text'] = cleanText(df['full_text'])\n",
    "    df['sentiment_distribution'] = df['full_text'].apply(lambda x: analyser.polarity_scores(x))\n",
    "    df['sentiment'] = df['sentiment_distribution'].apply(lambda x: x['compound'])\n",
    "    df.to_csv(f'tweets/tweets_processed/coronavirus_{state}_tweets_processed', index=False)\n",
    "    print(f'Finished processing and analyzing sentiment for {state}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
