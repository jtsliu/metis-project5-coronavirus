{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T19:18:42.844771Z",
     "start_time": "2020-03-19T19:18:42.375017Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T19:19:02.942591Z",
     "start_time": "2020-03-19T19:18:42.856362Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('tweets/coronavirus_massachusetts_tweets.txt', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T20:42:44.307557Z",
     "start_time": "2020-03-19T20:42:44.305159Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name = 'tweets/coronavirus_massachusetts_tweets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T20:42:57.433063Z",
     "start_time": "2020-03-19T20:42:57.429111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coronavirus_massachusetts_tweets.txt'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T20:31:05.217860Z",
     "start_time": "2020-03-19T20:31:05.213457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'indices': [80, 88], 'text': 'COVID19'},\n",
       " {'indices': [89, 101], 'text': 'coronavirus'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1020]['entities']['hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T19:19:44.253420Z",
     "start_time": "2020-03-19T19:19:44.145270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 94157 entries, 0 to 94156\n",
      "Data columns (total 30 columns):\n",
      " #   Column                     Non-Null Count  Dtype              \n",
      "---  ------                     --------------  -----              \n",
      " 0   contributors               0 non-null      float64            \n",
      " 1   coordinates                227 non-null    object             \n",
      " 2   created_at                 94157 non-null  datetime64[ns, UTC]\n",
      " 3   display_text_range         94157 non-null  object             \n",
      " 4   entities                   94157 non-null  object             \n",
      " 5   favorite_count             94157 non-null  int64              \n",
      " 6   favorited                  94157 non-null  bool               \n",
      " 7   full_text                  94157 non-null  object             \n",
      " 8   geo                        227 non-null    object             \n",
      " 9   id                         94157 non-null  int64              \n",
      " 10  id_str                     94157 non-null  int64              \n",
      " 11  in_reply_to_screen_name    0 non-null      float64            \n",
      " 12  in_reply_to_status_id      0 non-null      float64            \n",
      " 13  in_reply_to_status_id_str  0 non-null      float64            \n",
      " 14  in_reply_to_user_id        0 non-null      float64            \n",
      " 15  in_reply_to_user_id_str    0 non-null      float64            \n",
      " 16  is_quote_status            94157 non-null  bool               \n",
      " 17  lang                       94157 non-null  object             \n",
      " 18  metadata                   94157 non-null  object             \n",
      " 19  place                      5633 non-null   object             \n",
      " 20  possibly_sensitive         71676 non-null  float64            \n",
      " 21  retweet_count              94157 non-null  int64              \n",
      " 22  retweeted                  94157 non-null  bool               \n",
      " 23  source                     94157 non-null  object             \n",
      " 24  truncated                  94157 non-null  bool               \n",
      " 25  user                       94157 non-null  object             \n",
      " 26  extended_entities          14741 non-null  object             \n",
      " 27  quoted_status              7105 non-null   object             \n",
      " 28  quoted_status_id           7105 non-null   float64            \n",
      " 29  quoted_status_id_str       7105 non-null   float64            \n",
      "dtypes: bool(4), datetime64[ns, UTC](1), float64(9), int64(4), object(12)\n",
      "memory usage: 19.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "created_at = datetime64\n",
    "display_text_range = list -> int64\n",
    "favorite_count = int64\n",
    "full_text = string\n",
    "id_str = int64\n",
    "possibly_sensitive = float64\n",
    "retweet_count = int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T20:12:07.230180Z",
     "start_time": "2020-03-19T20:12:07.211618Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_df = df.drop(['contributors', 'display_text_range', \n",
    "#                   'favorite_count', 'favorited', 'id', \n",
    "#                   'in_reply_to_screen_name', 'in_reply_to_status_id',\n",
    "#                   'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
    "#                   'possibly_sensitive', 'retweet_count', 'retweeted', \n",
    "#                   'lang', 'metadata', 'truncated', 'user'], axis=1)\n",
    "\n",
    "new_df = df[['id_str', 'created_at', 'full_text', 'possibly_sensitive',\n",
    "             'display_text_range', 'favorite_count', 'retweet_count']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T20:12:07.789769Z",
     "start_time": "2020-03-19T20:12:07.735070Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df['character_count'] = new_df['display_text_range'].apply(lambda x: x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T20:38:39.452588Z",
     "start_time": "2020-03-19T20:38:05.582869Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df = pd.concat([new_df, df['entities'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T20:39:52.971350Z",
     "start_time": "2020-03-19T20:39:52.795036Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df['hashtags_count'] = new_df['hashtags'].apply(lambda x: len(x))\n",
    "new_df['symbols_count'] = new_df['symbols'].apply(lambda x: len(x))\n",
    "new_df['urls_count'] = new_df['urls'].apply(lambda x: len(x))\n",
    "new_df['user_mentions_count'] = new_df['user_mentions'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T20:45:32.383763Z",
     "start_time": "2020-03-19T20:45:32.276017Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df.drop(['hashtags', 'symbols', 'urls', \n",
    "             'user_mentions', 'media'],\n",
    "            axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T22:44:25.590270Z",
     "start_time": "2020-03-19T22:44:25.581683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_str', 'created_at', 'full_text', 'possibly_sensitive',\n",
       "       'display_text_range', 'favorite_count', 'retweet_count',\n",
       "       'character_count', 'hashtags_count', 'symbols_count', 'urls_count',\n",
       "       'user_mentions_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T01:42:38.219202Z",
     "start_time": "2020-03-19T01:42:38.180017Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df = df[['id_str', 'created_at', 'full_text',\n",
    "             'entities', 'extended_entities',\n",
    "             'coordinates', 'geo', 'place', 'source', 'is_quote_status',\n",
    "             'quoted_status', 'quoted_status_id', 'quoted_status_id_str']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T19:15:10.678739Z",
     "start_time": "2020-03-19T19:15:10.671570Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df.possibly_sensitive.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T01:42:39.209610Z",
     "start_time": "2020-03-19T01:42:38.272380Z"
    }
   },
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T01:42:39.217855Z",
     "start_time": "2020-03-19T01:42:39.212341Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "# [stop_words.append(x.replace('\\'', '')) for x in stop_words if \"'\" in x]\n",
    "stop_words.extend(['coronavirus', 'covid', 'covidー', 'coronavirusoutbreak', 'coronaviruspandemic'])\n",
    "stop_words = set(stop_words)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T01:42:39.226447Z",
     "start_time": "2020-03-19T01:42:39.219977Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "    preprocesser = lambda x: p.clean(x) #removes URL, @Mentions, Emojis, Smileys\n",
    "    \n",
    "    # prevent acronym for United States from losing meaning\n",
    "    expand_us = lambda x: x.replace('U.S.', 'United States')\n",
    "    \n",
    "    # remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    remove_punctuation = lambda x: x.translate(table)\n",
    "    \n",
    "    # remove numbers\n",
    "    text_nonum = lambda x: re.sub(r'\\d+', '', x)\n",
    "    \n",
    "    # convert all letters to lowercase\n",
    "    text_lower = lambda x: x.lower()\n",
    "\n",
    "    # substitute multiple spaces with single space\n",
    "    text_nospaces = lambda x: re.sub(r'\\s+', ' ', x, flags=re.I)\n",
    "\n",
    "    # remove all single characters\n",
    "    text_single = lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)\n",
    "    \n",
    "    # tokenize words\n",
    "    tokenize = lambda x: word_tokenize(x)\n",
    "    remove_stop = lambda x: [w for w in x if w not in stop_words]\n",
    "#     lemmatize_tweet = lambda x: [lemmatizer.lemmatize(word) for word in x]\n",
    "    create_string = lambda x: ' '.join(x)\n",
    "    \n",
    "    for function in [preprocesser, expand_us, remove_punctuation, \n",
    "                     text_nonum, text_lower, text_nospaces, \n",
    "                     text_single,\n",
    "                     tokenize, remove_stop, \n",
    "                     # lemmatize_tweet, \n",
    "                     create_string\n",
    "                    ]:\n",
    "        text = text.map(function)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T01:42:56.897137Z",
     "start_time": "2020-03-19T01:42:39.228127Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df['full_text'] = cleanText(new_df.full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T01:42:56.940221Z",
     "start_time": "2020-03-19T01:42:56.899666Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T01:42:56.944176Z",
     "start_time": "2020-03-19T01:42:56.942116Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.iloc[10].full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T01:42:56.947980Z",
     "start_time": "2020-03-19T01:42:56.945984Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_df.iloc[10].full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T01:42:59.066230Z",
     "start_time": "2020-03-19T01:42:56.949710Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import KMeans \n",
    "\n",
    "import umap.umap_ as umap\n",
    "from yellowbrick.text import UMAPVisualizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T01:42:59.072370Z",
     "start_time": "2020-03-19T01:42:59.068203Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\", topic_names[ix], \"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                         for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:06:48.568306Z",
     "start_time": "2020-03-19T02:06:48.561466Z"
    }
   },
   "outputs": [],
   "source": [
    "index_names = [f'component_{x}' for x in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:06:58.699536Z",
     "start_time": "2020-03-19T02:06:55.483653Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf, dtm = createCustomTFIDFvectorizer(new_df, n_range=(1,2), stop = stop_words)\n",
    "\n",
    "# don't use TF-IDF for tweets\n",
    "# use CV because it's a small doc\n",
    "# suggestion - binary = True ~ can throw in a lot of noise\n",
    "cv = CountVectorizer(ngram_range = (1,1), stop_words = stop_words, min_df=3, max_df=0.85)\n",
    "data_dtm = cv.fit_transform(new_df.full_text)\n",
    "data_dtm = pd.DataFrame(data_dtm.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = new_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:06:59.776478Z",
     "start_time": "2020-03-19T02:06:59.402462Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:30:07.624743Z",
     "start_time": "2020-03-19T02:07:00.259273Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components = 5, random_state = 42)\n",
    "doc_topic_nmf = nmf_model.fit_transform(data_dtm)\n",
    "\n",
    "topic_word_nmf = pd.DataFrame(nmf_model.components_.round(3),\n",
    "                                  index=index_names,\n",
    "                                  columns=cv.get_feature_names())\n",
    "\n",
    "# doc_topic_nmf_chosen = performCustomNMF(tf, dtm, index_names = index_names)\n",
    "#len(doc_topic_nmf_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:56:20.828316Z",
     "start_time": "2020-03-19T02:56:20.760911Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = cv.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(5):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict);\n",
    "\n",
    "get_nmf_topics(nmf_model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T03:00:12.428315Z",
     "start_time": "2020-03-19T02:58:50.020080Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_values = nmf_model.transform(data_dtm)\n",
    "new_df['Topic'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:57:50.013301Z",
     "start_time": "2020-03-19T02:57:49.976997Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_word_nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T02:56:35.589133Z",
     "start_time": "2020-03-19T02:56:35.557710Z"
    }
   },
   "outputs": [],
   "source": [
    "display_topics(nmf_model, cv.get_feature_names(), 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T00:06:53.068480Z",
     "start_time": "2020-03-19T00:06:51.921824Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=42).fit(doc_topic_nmf)\n",
    "cluster_topic_nmf = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-19T00:11:36.205746Z",
     "start_time": "2020-03-19T00:07:40.569792Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "docs = doc_topic_nmf\n",
    "labels = cluster_topic_nmf\n",
    "\n",
    "umap = UMAPVisualizer(metric ='euclidean', colormap = 'Set1', random_state=42)\n",
    "umap.fit(docs, labels)\n",
    "umap.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performCustomNMF(vec, dtm, index_names = index_names):\n",
    "    nmf_model = NMF(n_components = 10, random_state = 42)\n",
    "    doc_topic_nmf = nmf_model.fit_transform(dtm)\n",
    "    \n",
    "    print('The mean of individual topic weights over all documents is:')\n",
    "    mean = np.mean(doc_topic_nmf, axis=0)\n",
    "    print(mean)\n",
    "    \n",
    "    print('---')\n",
    "    print('The variance of individual topic weights over all documents is:')\n",
    "    variance = np.var(doc_topic_nmf, axis=0)\n",
    "    print(variance)\n",
    "    \n",
    "    topic_word_nmf = pd.DataFrame(nmf_model.components_.round(3),\n",
    "                                  index=index_names,\n",
    "                                  columns=vec.get_feature_names())\n",
    "    print('---')\n",
    "    print('The topics are:')\n",
    "    display_topics(nmf_model, vec.get_feature_names(), 60)\n",
    "    \n",
    "    return_doc_topic_nmf = doc_topic_nmf.copy()\n",
    "    kmeans = KMeans(n_clusters=num_features, random_state=random_state).fit(doc_topic_nmf)\n",
    "    cluster_topic_nmf = kmeans.labels_\n",
    "    best_topic_nmf = [x.argmax() for x in doc_topic_nmf]\n",
    "    \n",
    "    # UMAP Visualization Option 1 -------------------------------------------\n",
    "    plt.figure(figsize=(15,10))\n",
    "    docs = doc_topic_nmf\n",
    "    labels = best_topic_nmf\n",
    "\n",
    "    umap = UMAPVisualizer(metric ='euclidean', colormap = 'Set1', random_state=random_state)\n",
    "    umap.fit(docs, labels)\n",
    "    umap.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_one(docs):\n",
    "    \"\"\"\n",
    "    Cleans tweet text so that it is in a form suitable for topic modeling.\n",
    "    ---\n",
    "    :param docs: Series of documents to be processed.\n",
    "    :return: Series of processed texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # remove URLs and hyperlinks\n",
    "    text_nourl = lambda x: re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|' \\\n",
    "                                  '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', x)\n",
    "\n",
    "    # remove @ names\n",
    "    text_noname = lambda x: re.sub('(@[A-Za-z0-9_]+)', '', x)\n",
    "\n",
    "    # remove hashtags\n",
    "    text_nohash = lambda x: re.sub('(#[A-Za-z0-9_]+)', '', x)\n",
    "\n",
    "    # remove numbers\n",
    "    text_nonum = lambda x: re.sub(r'\\d+', '', x)\n",
    "\n",
    "    # remove the new line character\n",
    "    text_nonewline = lambda x: re.sub('\\n', '', x)\n",
    "\n",
    "    # remove punctuation\n",
    "    text_nopunct = lambda x: ''.join([char for char in x if char not in string.punctuation])\n",
    "\n",
    "    # convert all letters to lowercase\n",
    "    text_lower = lambda x: x.lower()\n",
    "\n",
    "    # substitute multiple spaces with single space\n",
    "    text_nospaces = lambda x: re.sub(r'\\s+', ' ', x, flags=re.I)\n",
    "\n",
    "    # remove all single characters\n",
    "    text_single = lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)\n",
    "\n",
    "    # apply all cleaning functions to input text\n",
    "    for clean_func in [text_nourl, text_noname, text_nohash, text_nonum, text_nonewline, \\\n",
    "                       text_nopunct, text_lower, text_nospaces, text_single]:\n",
    "        docs = docs.map(clean_func)\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def clean_text_two(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "\n",
    "    # remove repeating letters\n",
    "    counter = 0\n",
    "    ascii_lowercase = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "                      'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    # replace 'aaaaaaaahhhhh' with 'aahh'\n",
    "    for letter in ascii_lowercase:\n",
    "        for row_idx, doc in enumerate(docs):\n",
    "            for word_idx, word in enumerate(doc):\n",
    "                original_word = word\n",
    "                while word != word.replace(letter*3, letter*2):\n",
    "                    word = word.replace(letter*3, letter*2)\n",
    "                    docs[row_idx][word_idx] = word\n",
    "    return docs\n",
    "\n",
    "\n",
    "def clean_text_three(docs):\n",
    "    \"\"\"\n",
    "    ---\n",
    "    :param docs: Pandas series of texts to pre-process.\n",
    "    :return: Pandas series of cleaned text.\n",
    "    \"\"\"\n",
    "    # Setting standard english stopwords + custom stopwords\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    STOPWORDS.remove(\"not\")\n",
    "    stop_words = ['samsung', 'samsungs', 'galayxs', 'galaxy', 's ', ' s', 'plus', 'ultra', 'z', 'flip', 'unpacked']\n",
    "\n",
    "    clean_text = []\n",
    "    for tweet in docs:\n",
    "        new_tweet = []\n",
    "        for word in tweet.split():\n",
    "            if (word not in STOPWORDS) and (word not in stop_words) and (word not in string.punctuation):\n",
    "                new_tweet.append(word)\n",
    "        clean_text.append(' '.join(new_tweet))\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def clean_text_four(docs):\n",
    "    \"\"\"\n",
    "    Cleans tweet text so that it is in a form suitable for topic modeling.\n",
    "    ---\n",
    "    :param docs: Series of documents to be processed.\n",
    "    :return: Series of processed texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # lemmatize tweets\n",
    "    wordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lemmatized_tweets = []\n",
    "    for text in docs:\n",
    "        try:\n",
    "            lemmatized_tweets.append(wordNetLemmatizer.lemmatize(text))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return lemmatized_tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
