{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Tweepy and Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tweepy\n",
    "#!pip install jsonpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T18:24:19.195747Z",
     "start_time": "2020-03-17T18:24:19.040903Z"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# Create credentials.py in working directory with \n",
    "# your application's key and secret. API_KEY and API_SECRET\n",
    "from credentials import *\n",
    "\n",
    "# -or-\n",
    "# Type in credentials below\n",
    "# API_KEY = 'string'\n",
    "# API_SECRET = 'string'\n",
    "\n",
    "auth = tweepy.AppAuthHandler(API_KEY, API_SECRET)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import linecache\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "import jsonpickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States to Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = ['california', 'colorado', 'florida', \n",
    "              'georgia', 'idaho', 'illinois', \n",
    "              'louisiana', 'massachusetts', 'newyork',\n",
    "              'tennessee', 'texas', 'washington']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracker\n",
    "Tracked most recent tweet scraped, number of tweets scraped, and location for geofence used for that state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracker = {}\n",
    "# I used linecache to get the max_id from my previously scraped tweets\n",
    "# for state in state_list:\n",
    "#     tracker[state] = {'max_id': linecache.getline(f'tweets/tweets_compressed/coronavirus_{state}_tweets.txt',2).split(',')[0],\n",
    "#                       'downloaded': 0,\n",
    "#                       'tweetLocation': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = {'california': {'max_id': '',\n",
    "          'downloaded': 0,\n",
    "          'tweetLocation': '36.116,-119.682,300mi'},\n",
    "         'colorado': {'max_id': '',\n",
    "          'downloaded': 0,\n",
    "          'tweetLocation': '39.060,-105.311,200mi'},\n",
    "         'florida': {'max_id': '',\n",
    "          'downloaded': 0,\n",
    "          'tweetLocation': '27.766,-81.687,225mi'},\n",
    "         'georgia': {'max_id': '',\n",
    "          'downloaded': 0,\n",
    "          'tweetLocation': '32.781,-83.334,150mi'},\n",
    "         'idaho': {'max_id': '',\n",
    "          'downloaded': 0,\n",
    "          'tweetLocation': '44.241,-114.479,200mi'},\n",
    "         'illinois': {'max_id': '',\n",
    "          'downloaded': 0,\n",
    "          'tweetLocation': '40.350,-88.986,150mi'},\n",
    "         'louisiana': {'max_id': '',\n",
    "          'downloaded': 0,\n",
    "          'tweetLocation': '31.170,-91.868,150mi'},\n",
    "         'massachusetts': {'max_id': '',\n",
    "          'downloaded': 0,\n",
    "          'tweetLocation': '42.230,-71.530,100mi'},\n",
    "         'newyork': {'max_id': '',\n",
    "          'downloaded': 0,\n",
    "          'tweetLocation': '40.700,-73.974,50mi'},\n",
    "         'tennessee': {'max_id': '',\n",
    "          'downloaded': 0,\n",
    "          'tweetLocation': '35.748,-86.692,200mi'},\n",
    "         'texas': {'max_id': '',\n",
    "          'downloaded': 0,\n",
    "          'tweetLocation': '31.527,-99.524,350mi'},\n",
    "         'washington': {'max_id': '',\n",
    "          'downloaded': 0,\n",
    "          'tweetLocation': '47.401,-121.491,200mi'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(tracker, open('tweets/tweets_tracker.txt', 'wb'))\n",
    "tracker = pickle.load(open('tweets/tweets_tracker.txt', 'rb'))\n",
    "error_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape To API's Limit\n",
    "Twitter only allows to scrape Tweets no older than 7 days, uncommenting this will scrape until that point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T23:53:51.084223Z",
     "start_time": "2020-03-18T23:53:37.265340Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code modified from https://bhaskarvk.github.io/2015/01/how-to-use-twitters-search-rest-api-most-effectively./\n",
    "\n",
    "# searchQuery = 'coronavirus -filter:replies -filter:retweets'  # search query can use filters from advanced twitter searching\n",
    "# tweetLocation = '30.976,112.271,275mi' # latitude, longitude, radius of circle from point in mi or km\n",
    "# tweetLang = 'en' # scraping only english tweets\n",
    "\n",
    "# maxTweets = 10000000 # some arbitrary large number\n",
    "# tweetsPerQry = 100  # this is the max the API permits\n",
    "# fName = f'coronavirus_hubeichina_tweets.txt' # file name\n",
    "\n",
    "# # If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "# # else default to no lower limit, go as far back as API allows\n",
    "# sinceId = None\n",
    "\n",
    "# # If results only below a specific ID are, set max_id to that ID.\n",
    "# # else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "# max_id = -1\n",
    "\n",
    "# tweetCount = 0\n",
    "# print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "# with open(fName, 'w') as f:\n",
    "#     while tweetCount < maxTweets:\n",
    "#         try:\n",
    "#             if (max_id <= 0):\n",
    "#                 if (not sinceId):\n",
    "#                     new_tweets = api.search(q=searchQuery, geocode=tweetLocation, \n",
    "#                                             lang=tweetLang, count=tweetsPerQry,\n",
    "#                                             tweet_mode='extended')\n",
    "#                 else:\n",
    "#                     new_tweets = api.search(q=searchQuery, geocode=tweetLocation, \n",
    "#                                             lang=tweetLang, count=tweetsPerQry,\n",
    "#                                             tweet_mode='extended',\n",
    "#                                             since_id=sinceId)\n",
    "#             else:\n",
    "#                 if (not sinceId):\n",
    "#                     new_tweets = api.search(q=searchQuery, geocode=tweetLocation, \n",
    "#                                             lang=tweetLang, count=tweetsPerQry,\n",
    "#                                             tweet_mode='extended',\n",
    "#                                             max_id=str(max_id - 1))\n",
    "#                 else:\n",
    "#                     new_tweets = api.search(q=searchQuery, geocode=tweetLocation, \n",
    "#                                             lang=tweetLang, count=tweetsPerQry,\n",
    "#                                             tweet_mode='extended',\n",
    "#                                             max_id=str(max_id - 1),\n",
    "#                                             since_id=sinceId)\n",
    "#             if not new_tweets:\n",
    "#                 print(\"No more tweets found\")\n",
    "#                 break\n",
    "#             for tweet in new_tweets:\n",
    "#                 f.write(jsonpickle.encode(tweet._json, unpicklable=False) +\n",
    "#                         '\\n')\n",
    "#             tweetCount += len(new_tweets)\n",
    "#             print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "#             max_id = new_tweets[-1].id\n",
    "#         except tweepy.TweepError as e:\n",
    "#             # Just exit if any error\n",
    "#             print(\"some error : \" + str(e))\n",
    "#             break\n",
    "\n",
    "# print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to scrape any new tweets given the proper start_id from the previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeNewTweets(state, tweetLocation, start_id, date):\n",
    "    ''' scrapes new tweets using Twitter API \n",
    "    \n",
    "    params\n",
    "    ---\n",
    "    state: file naming and tracking purposes\n",
    "    tweetLocation: geofence for scraping tweets, must be in string of lat, long, radius (mi or km appended)\n",
    "        Example: '30.976,112.271,275mi'\n",
    "    start_id: scrape until it reaches this ID\n",
    "    date: file naming purposes\n",
    "    '''\n",
    "    searchQuery = 'coronavirus -filter:replies -filter:retweets'  # search query can use filters from advanced twitter searching\n",
    "    tweetLocation = tweetLocation # latitude, longitude, radius of circle from point in mi or km\n",
    "    tweetLang = 'en'\n",
    "\n",
    "    maxTweets = 10000000 # Some arbitrary large number\n",
    "    tweetsPerQry = 100  # this is the max the API permits\n",
    "    fName = f'tweets/tweets_{date}/coronavirus_{state}_tweets_{date}.txt' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "    # If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "    # else default to no lower limit, go as far back as API allows\n",
    "    sinceId = start_id\n",
    "\n",
    "    # If results only below a specific ID are, set max_id to that ID.\n",
    "    # else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "    max_id = -1\n",
    "\n",
    "    tweetCount = 0\n",
    "    print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "    with open(fName, 'w') as f:\n",
    "        while tweetCount < maxTweets:\n",
    "            try:\n",
    "                if (max_id <= 0):\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.search(q=searchQuery, geocode=tweetLocation, \n",
    "                                                lang=tweetLang, count=tweetsPerQry,\n",
    "                                                tweet_mode='extended')\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=searchQuery, geocode=tweetLocation, \n",
    "                                                lang=tweetLang, count=tweetsPerQry,\n",
    "                                                tweet_mode='extended',\n",
    "                                                since_id=sinceId)\n",
    "                else:\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.search(q=searchQuery, geocode=tweetLocation, \n",
    "                                                lang=tweetLang, count=tweetsPerQry,\n",
    "                                                tweet_mode='extended',\n",
    "                                                max_id=str(max_id - 1))\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=searchQuery, geocode=tweetLocation, \n",
    "                                                lang=tweetLang, count=tweetsPerQry,\n",
    "                                                tweet_mode='extended',\n",
    "                                                max_id=str(max_id - 1),\n",
    "                                                since_id=sinceId)\n",
    "                if not new_tweets:\n",
    "                    print(\"No more tweets found\")\n",
    "                    break\n",
    "                for tweet in new_tweets:\n",
    "                    f.write(jsonpickle.encode(tweet._json, unpicklable=False) +\n",
    "                            '\\n')\n",
    "                tweetCount += len(new_tweets)\n",
    "                print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                if max_id == -1:\n",
    "                    max_id = new_tweets[-1].id\n",
    "                    tracker[state]['max_id'] = max_id\n",
    "                else:\n",
    "                    max_id = new_tweets[-1].id\n",
    "            except tweepy.TweepError as e:\n",
    "                # Just exit if any error\n",
    "                error_counter += 1\n",
    "                print(\"some error : \" + str(e))\n",
    "                break\n",
    "\n",
    "    print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))\n",
    "    tracker[state]['downloaded'] += tweetCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new folder as to not override previous scrapes\n",
    "today = datetime.today().strftime('%Y_%m_%d')\n",
    "\n",
    "if not os.path.exists(f'tweets/tweets_{today}'):\n",
    "    os.makedirs(f'tweets/tweets_{today}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch running per state\n",
    "for state in state_list:\n",
    "    print(f'Starting to scrape for {state}')\n",
    "    scrapeNewTweets(state, tracker[state]['tweetLocation'],  tracker[state]['max_id'], today)\n",
    "\n",
    "pickle.dump(tracker, open('tweets/tweets_tracker.txt', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What’s more using the initial values for max_id and/or since_id you can fetch results to and from arbitrary IDs. This is really helpful if you want to the program repeatedly to fetch newer results since last run. Just look up the max ID (the ID of the first line) from the previous run and set that to since_id for the next run. If you’ve to stop your program before exhausting all the possible results and rerun it again to fetch the remaining results, you can look up the min ID (the ID of the last line) and pass that as max_id for the next run to start from that ID and below.\n",
    "* max_id (first tweet) from previous run -> since_id = \n",
    "* min_id (last tweet) -> max_id for new run if starting from where you left off = \n",
    "\n",
    "**California**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 298499 tweets**\n",
    "\n",
    "* searchQuery = 'covid19'\n",
    "* **Downloaded 57200 tweets**\n",
    "* tweetLocation = '36.116,-119.682,300mi'\n",
    "\n",
    "**Florida**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 102359 tweets**\n",
    "\n",
    "* searchQuery = 'covid19'\n",
    "* **Downloaded --- tweets**\n",
    "* tweetLocation = '27.766,-81.687,225mi'\n",
    "\n",
    "**Massachusetts**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 94157 tweets**\n",
    "* tweetLocation = '42.230,-71.530,100mi'\n",
    "\n",
    "**New York**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 293528 tweets**\n",
    "* tweetLocation =  '40.700,-73.974,50mi'\n",
    "\n",
    "**Washington**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 85792 tweets**\n",
    "* tweetLocation = '47.401,-121.491,200mi'\n",
    "\n",
    "**Tennessee**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 139487 tweets**\n",
    "* tweetLocation = '35.748,-86.692,200mi'\n",
    "\n",
    "**Texas**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 196317 tweets**\n",
    "* tweetLocation = '31.527,-99.524,350mi'\n",
    "\n",
    "**Louisiana**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 54235 tweets**\n",
    "* tweetLocation = '31.170,-91.868,150mi'\n",
    "\n",
    "**Illinois**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 140102 tweets**\n",
    "* tweetLocation = '40.350,-88.986,150mi'\n",
    "\n",
    "**Colorado**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 30059 tweets**\n",
    "* tweetLocation = '39.060,-105.311,200mi'\n",
    "\n",
    "**Georgia**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 101940 tweets**\n",
    "* tweetLocation = '32.781,-83.334,150mi'\n",
    "\n",
    "**Idaho**\n",
    "* searchQuery = 'coronavirus'\n",
    "* **Downloaded 6882 tweets**\n",
    "* tweetLocation = '44.241,-114.479,200mi'\n",
    "\n",
    "Countries\n",
    "---\n",
    "**Italy**\n",
    "* **Downloaded 13835 tweets**\n",
    "* tweetLocation = '41.872,12.567,275mi'\n",
    "\n",
    "**Hubei, China**\n",
    "* **Downloaded 1359 tweets**\n",
    "* tweetLocation = '30.976,112.271,275mi'\n",
    "\n",
    "**South Korea**\n",
    "Latitude\tLongitude\n",
    "35.908\t127.767\n",
    "\n",
    "**France**\n",
    "Latitude\tLongitude\n",
    "46.228\t2.214\n",
    "\n",
    "\n",
    "**Spain**\n",
    "Latitude\tLongitude\n",
    "40.464\t-3.749\n",
    "\n",
    "**Germany**\n",
    "Latitude\tLongitude\n",
    "51.166\t10.452\n",
    "\n",
    "Retweets Included Below\n",
    "--- \n",
    "**California**\n",
    "* Downloaded 670881 tweets\n",
    "* max_id (first tweet) from previous run -> since_id = \"1239943180579000320\" \n",
    "* min_id (last tweet) -> max_id for new run if starting from where you left off = \"1239289843491823616\"\n",
    "* searchQuery = 'coronavirus'\n",
    "* tweetLocation = '36.116,-119.682,300mi'\n",
    "* tweetLang = 'en'\n",
    "* fName = 'coronavirus_california_tweets.txt'\n",
    "\n",
    "**Florida**\n",
    "* Downloaded\n",
    "* max_id (first tweet) from previous run -> since_id = \n",
    "* min_id (last tweet) -> max_id for new run if starting from where you left off = \n",
    "* searchQuery = 'coronavirus'\n",
    "* tweetLocation = '27.766,-81.687,225mi'\n",
    "* tweetLang = 'en'\n",
    "* fName = 'coronavirus_florida_tweets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
